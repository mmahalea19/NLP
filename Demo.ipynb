{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Demo.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "pycharm-1538f835",
   "language": "python",
   "display_name": "PyCharm (NLP)"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/maha/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /home/maha/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize ,RegexpTokenizer\n",
    "import string as string\n",
    "def removeWords(phrases,options):\n",
    "  print(\"Initial\")\n",
    "  print(phrases)\n",
    "  if(\"link\" in options):\n",
    "    phrases=[re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", phrase) for phrase in phrases]\n",
    "    print(\"After link\")\n",
    "    print(phrases)\n",
    "  if \"symbol\" in options:\n",
    "    whitelist = string.ascii_letters + string.digits + ' '\n",
    "    symbolRemoved=[]\n",
    "    for phrase in phrases:\n",
    "      phrase1=''.join(list(map(lambda cha: cha if cha  in whitelist else ' ',phrase)))\n",
    "      symbolRemoved.append(phrase1)\n",
    "    print(\"After symbol removal\")\n",
    "\n",
    "    phrases=symbolRemoved\n",
    "    print(phrases)\n",
    "\n",
    "\n",
    "  tokenized=[word_tokenize(phrase) for phrase in phrases] #split in individual words\n",
    "  # print(tokenized)\n",
    "  if \"stopword\" in options:\n",
    "    stop_words = set(stopwords.words('english')) #get set of stopwords\n",
    "    filtered=list(map(lambda phrase:[w for w in phrase if w not in stop_words],tokenized)) # remove words that appear in the stopwords set\n",
    "    print(\"After stopword removal\")\n",
    "\n",
    "    tokenized=filtered\n",
    "    #phrases=''.join(filtered)\n",
    "    phrases=[' '.join(x) for x in filtered]\n",
    "    print(phrases)\n",
    "  return phrases\n",
    "  print(\"Done\")\n",
    "\n",
    "# removeWords([\"This is-a really nice day to. ? swim in the ocean\",\"Hello  @my friends https://mdshasdasdhas carte\"],[\"stopword\",\"symbol\",\"link\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "def make_Dictionary(train_dir):\n",
    "    emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]\n",
    "    all_words = []\n",
    "    for mail in emails:\n",
    "        with open(mail) as m:\n",
    "            for i,line in enumerate(m):\n",
    "                if i == 2:  #Body of email is only 3rd line of text file\n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    " \n",
    "    dictionary1 = Counter(all_words)\n",
    "    # Paste code for non-word removal here(code snippet is given below)\n",
    "    list_to_remove = list(dictionary1)\n",
    "    \n",
    "    for item in list_to_remove:\n",
    "        if item.isalpha() == False:\n",
    "            del dictionary1[item]\n",
    "        elif len(item) == 1:\n",
    "            del dictionary1[item]\n",
    "    dictionary1 = dictionary1.most_common(3000)\n",
    "    return dictionary1\n",
    "def reduceFeatures(raw,nrFeats):\n",
    "    #min max strategy, cut from both ends until the required number of features\n",
    "    x=True\n",
    "    while(raw.shape[1]>nrFeats):\n",
    "        sums=[sum(x) for x in zip(*raw)]\n",
    "        index=-1\n",
    "        if x:\n",
    "            index=np.argmin(sums)\n",
    "        else:\n",
    "            index=np.argmax(sums)\n",
    "        x=not x\n",
    "        raw=np.delete(raw,index,1)\n",
    "    return raw\n",
    "        \n",
    "        \n",
    "def extract_my_features(mail_dir,filter):#idf features\n",
    "    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "    lines=[]   \n",
    "    for fil in files:\n",
    "        with open(fil) as fi:\n",
    "            for i,line in enumerate(fi):\n",
    "                if i == 2:\n",
    "                    lines.append(line)\n",
    "    # lines=[\"the house had a tiny little mouse\",\n",
    "    #   \"the cat saw the mouse\",\n",
    "    #   \"the mouse ran away from the house\",\n",
    "    #   \"the cat finally ate the mouse\",\n",
    "    #   \"the end of the mouse story\"]\n",
    "    lines=removeWords(lines,filter)\n",
    "    tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    "     \n",
    "    # just send in all your docs here\n",
    "    tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(lines)\n",
    "    first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n",
    " \n",
    "# place tf-idf values in a pandas data frame\n",
    "    df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "    df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "    \n",
    "    return tfidf_vectorizer_vectors\n",
    "def LDA_PCA(raw,option,nr_components,labels=None):\n",
    "    if option is \"pca\":\n",
    "        pca = PCA(n_components=nr_components)\n",
    "        x_pca = pca.fit_transform(raw)\n",
    "        return x_pca\n",
    "    if option is \"lda\":\n",
    "        lda = LDA(n_components=nr_components)\n",
    "        x_lda=lda.fit_transform(raw,labels)\n",
    "        return x_lda\n",
    "        \n",
    "def extract_features(mail_dir,dictio,filter):#based of number of occurences of words from dictionary\n",
    "    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "    features_matrix = np.zeros((len(files),3000))\n",
    "    docID = 0;\n",
    "   \n",
    "    for fil in files:\n",
    "      with open(fil) as fi:\n",
    "        content = fi.read().splitlines()\n",
    "    \n",
    "        content=removeWords(content,filter)\n",
    "\n",
    "        for i,line in enumerate(content):\n",
    "          if i == 2:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "             \n",
    "              for i,d in enumerate(dictio):\n",
    "                if d[0] == word:\n",
    "                  wordID = i\n",
    "                  features_matrix[docID,wordID] = words.count(word)\n",
    "        docID = docID + 1\n",
    "    return features_matrix\n",
    "# \n",
    "# train_dir = 'Datasets/ling-spam/train-mails'\n",
    "# features=extract_my_features(train_dir)\n",
    "# features_array=features.toarray()\n",
    "# pca_feat=LDA_PCA(features_array,\"pca\",2)\n",
    "# train_labels = np.ones(20)\n",
    "# train_labels[0:9] = 0\n",
    "# lda_feat=LDA_PCA(features_array,\"lda\",2,train_labels)\n",
    "# reduced_features=reduceFeatures(features_array,10)\n",
    "# print(reduced_features)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Initial\n['Subject: re : 2 . 882 s - > np np', '', '> deat : sun , 15 dec 91 2 : 25 : 2 est > : michael < mmorse @ vm1 . yorku . ca > > subject : re : 2 . 864 query > > wlodek zadrozny ask \" anything interest \" > construction \" s > np np \" . . . second , > much relate : consider construction form > discuss list late reduplication ? > logical sense \" john mcnamara name \" tautologous thus , > level , indistinguishable \" , , here ? \" . \\' john mcnamara name \\' tautologous support those logic-base semantics irrelevant natural language . sense tautologous ? supplies value attribute follow attribute value . fact value name-attribute relevant entity \\' chaim shmendrik \\' , \\' john mcnamara name \\' false . tautology , . ( reduplication , either . )']\nAfter stopword removal\n['Subject : : 2 . 882 - > np np', '', \"> deat : sun , 15 dec 91 2 : 25 : 2 est > : michael < mmorse @ vm1 . yorku . ca > > subject : : 2 . 864 query > > wlodek zadrozny ask `` anything interest `` > construction `` > np np `` . . . second , > much relate : consider construction form > discuss list late reduplication ? > logical sense `` john mcnamara name `` tautologous thus , > level , indistinguishable `` , , ? `` . ' john mcnamara name ' tautologous support logic-base semantics irrelevant natural language . sense tautologous ? supplies value attribute follow attribute value . fact value name-attribute relevant entity ' chaim shmendrik ' , ' john mcnamara name ' false . tautology , . ( reduplication , either . )\"]\n",
      "Initial\n['Subject: s - > np + np', '', 'discussion s - > np + np remind ago read , source forget , critique newsmagazine \\' unique tendency write style , most writer overly \" cute \" . one item tersely put down follow : \" \\'s favorite : colon . \" - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - lee hartman ga5123 @ siucvmb . bitnet department foreign language southern illinoi university carbondale , il 62901 u . s . .']\nAfter stopword removal\n['Subject : - > np + np', '', \"discussion - > np + np remind ago read , source forget , critique newsmagazine ' unique tendency write style , writer overly `` cute `` . one item tersely put follow : `` 's favorite : colon . `` - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - lee hartman ga5123 @ siucvmb . bitnet department foreign language southern illinoi university carbondale , il 62901 u . . .\"]\n",
      "Initial\n['Subject: 2 . 882 s - > np np', '', '. . . \\'s much restrictive s - > np np . \\'s \" \" np pro quite over-restriction , .']\nAfter stopword removal\n['Subject : 2 . 882 - > np np', '', \". . . 's much restrictive - > np np . 's `` `` np pro quite over-restriction , .\"]\n",
      "Initial\n['Subject: 2 . 882 s - > np np', '', '. . . \\'s much restrictive s - > np np . \\'s \" \" np pro quite over-restriction , .']\nAfter stopword removal\n['Subject : 2 . 882 - > np np', '', \". . . 's much restrictive - > np np . 's `` `` np pro quite over-restriction , .\"]\n",
      "Initial\n['Subject: gent conference', '', '\" listserv \" international conference 1992 second circular : february 1992 literature analysis discourse special attention multicultural context tuesday 8 september - friday 11 september 1992 gent university , belgium write read literature , oral literary tradition , dialogic text , non-literary narrative , discourse theory , literature social practice , etc . , etc . , etc . keynote speaker : david birch ( murdoch , australium ) martin montgomery ( strathclyde , scotland ) elinor ochs ( lo angele , usa ) statement pala \\' s aims palum \\'s principal aim encourage cooperation between scholar teacher interest language / literary study . interest pala member wide , reflect papers pala conference . interest member include : stylistic , literary theory , teach language literature , critical linguistics , pragmatic , discour analysis , textual understand , rhetoric , narratology , semiotic approach text performance , sociolinguistic , cultural study , post-structuralist theory ; short , theme relevance study teach language literature role society . 1992 conference theme highlight currently expand field discour study , 1992 conference core theme \\' literature analysis discourse , special attention multicultural context \\' . paper cover interest wide process write read literature , analysis dialogic text , oral literary tradition , relationship between literary non-literary discourse , discourse theory literary communication social practice propose , those deal specifically write read literature multilingual / multicultural context . 1992 conference venue gent university city type ; campus , university building dot around town . conference session place hoveniersberg , overlook bovenschelde one quiet part town . programme conference session start morn wednesday full three day . envisage most participant arrive register tuesday even . our provisional programme : tuedsday 8 sept 15 . 0 onward : registration wednesday 9 sept 08 . 30 - 09 . 30 : late registration 09 . 45 : open conference 10 . 0 - 18 . 0 : conference session 18 . 30 : pre-book dinner 20 . 15 : drink reception thursday 10 sept 08 . 30 - 18 . 0 : conference session 18 . 30 : pala agm 20 . 0 : pre-book dinner friday 11 sept 08 . 30 - 17 . 0 : conference session 17 . 15 : wind-up session even : activity arrange continuous coffee , tea , etc . throughout conference session . accommodation room vermeylen student hall residence , couple hundr metre conference centre , available participant . possible book room several night either side conference date . price registration form include breakfast . unfortunately , double room available . prefer stay hotel , recommend arcade hotel ( nederkouter , 9000 gent ; tel . 32-91 - 25 . 7 . 7 ) , 10 minute \\' walk conference centre . alternatively , contact gent tourist office ( meersstraat 138 , 9000 gent ; tel . 32-91 - 25 . 35 . 55 ) . food breakfast serve overpoort , university eat complex next door vermeylen . lunch supper available conference participant , snack throughout day . single \\' conference dinner \\' , easier participant meet each , arrange dinner both wednesday thursday evening university restaurant . pre-book . staying gent gent ( population around 230 , 0 ) historic flemish city , first europe declare itself independent feudal control . plethora medieval vista bridge thus entitle compete bruge amsterdam title \\' venice north \\' . busy industrial city commercial administrative centre east flander . first language flemish / dutch ( depend one \\'s sociolinguistic viewpoint ) nearly every-body both english french least degree fluency . numerous restaurant , cafe pub near conference area ( include two vegetarian restaurant ) , many stay open small hour . price cheap northern european standard . those wish combine conference visit gent surround area , train less hour bruge , brussel , antwerp belgian coast . even ardenn pari within few hour . registration / queries attend conference , fill registration form return , payment , 1st . confirmation registration detail arrangement send third circular those register , enquiry , contact jim o\\'driscoll stef slembrouck seminarie voor engelse taalkunde , universiteit gent , rozier 44 , b-9000 gent , belgium ( tel : 32-91 - 64 . 37 . 88 / 89 / 90 ; fax : 32-91 - 64 . 41 . 95 ; e-mail pala92 @ engllang . rug . ac . ) . * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * pala 92 gent university registration form surname _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ first name ( s ) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ address _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ affiliation _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ participate conference enclose eurocheque ( arrange direct transfer pala account belgium ) cover : ( tick appropriate ) pala member conference fee ( bf 1000 ) _ _ _ _ _ _ non-member conference fee ( bf 2000 ) _ _ _ _ _ _ student conference fee ( bf 600 ) _ _ _ _ _ _ dinner 9th september ( bf 500 ) _ _ _ _ _ _ dinner 10th september ( bf 500 ) _ _ _ _ _ _ accommodation tue 8th september ( bf 525 ) _ _ _ _ _ _ accommodation wed 9th september ( bf 525 ) _ _ _ _ _ _ accommodation thu 10th september ( bf 525 ) _ _ _ _ _ _ accommodation frus 11th september ( bf 525 ) _ _ _ _ _ _ accommodation ( specify ) ( bf ) _ _ _ _ _ _ fee international money transfer cheque eurocheque * ( bf 300 ) _ _ _ _ _ _ therefore enclose ( transfer ) total bf _ _ _ _ _ _ lacto-vegetarian / vegan food dinner ( s ) book _ _ _ _ _ signature _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ please return pala conference 1992 , seminarie voor engelse taalkunde , universiteit gent , rozier 44 , b-9000 gent , belgium ( pala9 @ engllang . rug . ac . ) . final date registration 1st 1992 . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * . note payment must belgian franc . cheque payable \\' pala conference 1992 \\' . single eurocheque must bf 7 , 0 . international money transfer send vium \\' swift \\' , quote our bank \\'s swift number ( bbru bb 900 ) our account number : bbl 390-0959358 - 83 . problem either method payment , please contact organizer .']\nAfter stopword removal\n['Subject : gent conference', '', \"`` listserv `` international conference 1992 second circular : february 1992 literature analysis discourse special attention multicultural context tuesday 8 september - friday 11 september 1992 gent university , belgium write read literature , oral literary tradition , dialogic text , non-literary narrative , discourse theory , literature social practice , etc . , etc . , etc . keynote speaker : david birch ( murdoch , australium ) martin montgomery ( strathclyde , scotland ) elinor ochs ( lo angele , usa ) statement pala ' aims palum 's principal aim encourage cooperation scholar teacher interest language / literary study . interest pala member wide , reflect papers pala conference . interest member include : stylistic , literary theory , teach language literature , critical linguistics , pragmatic , discour analysis , textual understand , rhetoric , narratology , semiotic approach text performance , sociolinguistic , cultural study , post-structuralist theory ; short , theme relevance study teach language literature role society . 1992 conference theme highlight currently expand field discour study , 1992 conference core theme ' literature analysis discourse , special attention multicultural context ' . paper cover interest wide process write read literature , analysis dialogic text , oral literary tradition , relationship literary non-literary discourse , discourse theory literary communication social practice propose , deal specifically write read literature multilingual / multicultural context . 1992 conference venue gent university city type ; campus , university building dot around town . conference session place hoveniersberg , overlook bovenschelde one quiet part town . programme conference session start morn wednesday full three day . envisage participant arrive register tuesday even . provisional programme : tuedsday 8 sept 15 . 0 onward : registration wednesday 9 sept 08 . 30 - 09 . 30 : late registration 09 . 45 : open conference 10 . 0 - 18 . 0 : conference session 18 . 30 : pre-book dinner 20 . 15 : drink reception thursday 10 sept 08 . 30 - 18 . 0 : conference session 18 . 30 : pala agm 20 . 0 : pre-book dinner friday 11 sept 08 . 30 - 17 . 0 : conference session 17 . 15 : wind-up session even : activity arrange continuous coffee , tea , etc . throughout conference session . accommodation room vermeylen student hall residence , couple hundr metre conference centre , available participant . possible book room several night either side conference date . price registration form include breakfast . unfortunately , double room available . prefer stay hotel , recommend arcade hotel ( nederkouter , 9000 gent ; tel . 32-91 - 25 . 7 . 7 ) , 10 minute ' walk conference centre . alternatively , contact gent tourist office ( meersstraat 138 , 9000 gent ; tel . 32-91 - 25 . 35 . 55 ) . food breakfast serve overpoort , university eat complex next door vermeylen . lunch supper available conference participant , snack throughout day . single ' conference dinner ' , easier participant meet , arrange dinner wednesday thursday evening university restaurant . pre-book . staying gent gent ( population around 230 , 0 ) historic flemish city , first europe declare independent feudal control . plethora medieval vista bridge thus entitle compete bruge amsterdam title ' venice north ' . busy industrial city commercial administrative centre east flander . first language flemish / dutch ( depend one 's sociolinguistic viewpoint ) nearly every-body english french least degree fluency . numerous restaurant , cafe pub near conference area ( include two vegetarian restaurant ) , many stay open small hour . price cheap northern european standard . wish combine conference visit gent surround area , train less hour bruge , brussel , antwerp belgian coast . even ardenn pari within hour . registration / queries attend conference , fill registration form return , payment , 1st . confirmation registration detail arrangement send third circular register , enquiry , contact jim o'driscoll stef slembrouck seminarie voor engelse taalkunde , universiteit gent , rozier 44 , b-9000 gent , belgium ( tel : 32-91 - 64 . 37 . 88 / 89 / 90 ; fax : 32-91 - 64 . 41 . 95 ; e-mail pala92 @ engllang . rug . ac . ) . * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * pala 92 gent university registration form surname _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ first name ( ) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ address _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ affiliation _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ participate conference enclose eurocheque ( arrange direct transfer pala account belgium ) cover : ( tick appropriate ) pala member conference fee ( bf 1000 ) _ _ _ _ _ _ non-member conference fee ( bf 2000 ) _ _ _ _ _ _ student conference fee ( bf 600 ) _ _ _ _ _ _ dinner 9th september ( bf 500 ) _ _ _ _ _ _ dinner 10th september ( bf 500 ) _ _ _ _ _ _ accommodation tue 8th september ( bf 525 ) _ _ _ _ _ _ accommodation wed 9th september ( bf 525 ) _ _ _ _ _ _ accommodation thu 10th september ( bf 525 ) _ _ _ _ _ _ accommodation frus 11th september ( bf 525 ) _ _ _ _ _ _ accommodation ( specify ) ( bf ) _ _ _ _ _ _ fee international money transfer cheque eurocheque * ( bf 300 ) _ _ _ _ _ _ therefore enclose ( transfer ) total bf _ _ _ _ _ _ lacto-vegetarian / vegan food dinner ( ) book _ _ _ _ _ signature _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ please return pala conference 1992 , seminarie voor engelse taalkunde , universiteit gent , rozier 44 , b-9000 gent , belgium ( pala9 @ engllang . rug . ac . ) . final date registration 1st 1992 . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * . note payment must belgian franc . cheque payable ' pala conference 1992 ' . single eurocheque must bf 7 , 0 . international money transfer send vium ' swift ' , quote bank 's swift number ( bbru bb 900 ) account number : bbl 390-0959358 - 83 . problem either method payment , please contact organizer .\"]\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a4807a4d7606>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_Dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stopword\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mtrain_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_classifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifierArray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d9b8758afbea>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(mail_dir, dictio, filter)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremoveWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d9b8758afbea>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(mail_dir, dictio, filter)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremoveWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/IDEA/apps/PyCharm-P/ch-0/192.6817.19/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0;31m# if thread has a suspend flag, we suspend with a busy wait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydev_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSTATE_SUSPEND\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m                     \u001b[0;31m# No need to reset frame.f_trace to keep the same trace function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/IDEA/apps/PyCharm-P/ch-0/192.6817.19/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001b[0m in \u001b[0;36mdo_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;31m# IFDEF CYTHON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/IDEA/apps/PyCharm-P/ch-0/192.6817.19/helpers/pydev/pydevd.py\u001b[0m in \u001b[0;36mdo_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads_suspended_single_notification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify_thread_suspended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuspend_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_this_thread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuspend_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_this_thread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/IDEA/apps/PyCharm-P/ch-0/192.6817.19/helpers/pydev/pydevd.py\u001b[0m in \u001b[0;36m_do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_internal_commands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel_async_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_current_thread_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "from itertools import permutations \n",
    "from random import randint\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a dictionary of words with its frequency\n",
    "\n",
    "# Training SVM and Naive bayes classifier\n",
    "def train_classifiers(classifiers,train_matrix,train_labels):\n",
    "    for classifier in classifiers:\n",
    "        classifier.fit(train_matrix,train_labels)\n",
    "    return classifiers\n",
    "        \n",
    "def majority_voting(classifiers,classifier_labels,sample,true_labels):\n",
    "    results=[]\n",
    "    classifier_order=range(0,len(classifiers)-1);\n",
    "    k=3\n",
    "    combinations=permutations(classifier_order,k)\n",
    "    for index,i in enumerate(list(combinations)):\n",
    "        majority_vote=classifiers[i[0]].predict(sample)+classifiers[i[1]].predict(sample)+classifiers[i[2]].predict(sample);\n",
    "        final_result=np.round(majority_vote/k);\n",
    "        results.append(final_result)\n",
    "        print(\"The final result for the classifiers {}|{}|{}={}\".format(classifier_labels[i[0]],classifier_labels[i[1]],classifier_labels[i[2]],confusion_matrix(true_labels,final_result)))\n",
    "\n",
    "\n",
    "model1 = MultinomialNB()\n",
    "model2 = LinearSVC()\n",
    "model3 = tree.DecisionTreeClassifier()\n",
    "model4 = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "classifierArray=[model1,model2,model3,model4]\n",
    "classifierLabels=[\"Muntinomial\",\"LinearSVC\",\"Decision Tree\",\"Random Forest\"]\n",
    "\n",
    "\n",
    "train_dir = 'Datasets/ling-spam/train-mails'\n",
    "\n",
    "# Prepare feature vectors per training mail and its labels\n",
    " \n",
    "train_labels = np.ones(20)\n",
    "train_labels[0:9] = 0\n",
    "dictionary=make_Dictionary(train_dir) \n",
    "filters=[\"stopword\"]\n",
    "\n",
    "\n",
    "train_matrix = extract_features(train_dir,dictionary,filters)\n",
    " \n",
    "classifiers=train_classifiers(classifierArray,train_matrix,train_labels)\n",
    "\n",
    "# Test the unseen mails for Spam\n",
    "test_dir = './Datasets/ling-spam/test-mails/'\n",
    "test_matrix = extract_features(test_dir,dictionary,filters)\n",
    "test_labels = np.ones(20)\n",
    "test_labels[0:9] = 0\n",
    "result1 = model1.predict(test_matrix)\n",
    "result2 = model2.predict(test_matrix)\n",
    "print (confusion_matrix(test_labels,result1))\n",
    "print (confusion_matrix(test_labels,result2))\n",
    "majority_voting(classifierArray,classifierLabels,test_matrix,test_labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bara's\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "from nltk .tokenize import TweetTokenizer\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "\n",
    "DATASET_DIR = \"./Dataset/Enron_PRE/\"\n",
    "\n",
    "\n",
    "def find_urls(filename):\n",
    "    no_url = 0\n",
    "    with open(filename) as fi:\n",
    "        for i, line in enumerate(fi):\n",
    "            ## curently we accept spaces between http : //. Correct or not?\n",
    "            url = re.findall('http[s]?\\s?:\\s?/', line) # 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+] |[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "            if len(url) != 0:\n",
    "                no_url += 1\n",
    "    return no_url\n",
    "\n",
    "\n",
    "def find_mistakes(filename):\n",
    "    no_mistakes = 0\n",
    "    spell = SpellChecker()\n",
    "    tknzr = TweetTokenizer()\n",
    "    with open(filename) as fi:\n",
    "        for i, line in enumerate(fi):\n",
    "            line_tok = tknzr.tokenize(line)\n",
    "            for w in line_tok:\n",
    "                correction = spell.correction(w)\n",
    "                if correction == w:\n",
    "                    no_mistakes += 1\n",
    "    return no_mistakes\n",
    "\n",
    "def find_words(filename):\n",
    "    no_words = 0\n",
    "    tknzr = TweetTokenizer()\n",
    "    with open(filename) as fi:\n",
    "        for i, line in enumerate(fi):\n",
    "            line_tok = tknzr.tokenize(line)\n",
    "            no_words += len(line_tok)\n",
    "    return no_words\n",
    "\n",
    "\n",
    "def find_entities(filename):\n",
    "    no_entities = 0\n",
    "    tknzr = TweetTokenizer()\n",
    "    with open(filename) as fi:\n",
    "        for i, line in enumerate(fi):\n",
    "            line_tok = tknzr.tokenize(line)\n",
    "            ne_tree = nltk.ne_chunk(nltk.pos_tag(line_tok), binary=True)\n",
    "            named_entities = []\n",
    "            for tagged_tree in ne_tree:\n",
    "                if hasattr(tagged_tree, 'label'):\n",
    "                    entity_name = ' '.join(c[0] for c in tagged_tree.leaves())  #\n",
    "                    entity_type = tagged_tree.label()  # get NE category\n",
    "                    named_entities.append((entity_name, entity_type))\n",
    "\n",
    "            no_entities += len(named_entities)\n",
    "    return no_entities\n",
    "\n",
    "def find_pronouns(filename):\n",
    "    no_pronouns = 0\n",
    "    with open(filename) as fi:\n",
    "        for i, line in enumerate(fi):\n",
    "            blob = TextBlob(line)\n",
    "            blob.parse()\n",
    "            for w in blob.tags:\n",
    "                print(w[1])\n",
    "                if 'PRP' in w[1]:\n",
    "                    no_pronouns += 1\n",
    "    return no_pronouns\n",
    "\n",
    "def find_repetitions(filename):\n",
    "    no_repetitions = 0\n",
    "    tknzr = TweetTokenizer()\n",
    "    d = dict()\n",
    "    with open(filename) as fi:\n",
    "        for i, line in enumerate(fi):\n",
    "            line_tok = tknzr.tokenize(line)\n",
    "            for w in line_tok:\n",
    "                if w in d:\n",
    "                    no_repetitions += 1\n",
    "                else:\n",
    "                    d[w] = 1\n",
    "    return no_repetitions\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "class mock_class:\n",
    "    def predict(self,s):\n",
    "        return randint(0,1)\n",
    "fake1=mock_class()\n",
    "fake2=mock_class()\n",
    "fake3=mock_class()\n",
    "fake4=mock_class()\n",
    "classSet=[fake1,fake2,fake3,fake4]\n",
    "classLabels=range(1,4)\n",
    "x=3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ]
}