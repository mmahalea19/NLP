{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Demo.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "pycharm-1538f835",
   "language": "python",
   "display_name": "PyCharm (NLP)"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize ,RegexpTokenizer\n",
    "import string as string\n",
    "def removeWords(phrases,options):\n",
    "  print(\"Initial\")\n",
    "  print(phrases)\n",
    "  if(\"link\" in options):\n",
    "    phrases=[re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", phrase) for phrase in phrases]\n",
    "    print(\"After link\")\n",
    "    print(phrases)\n",
    "  if \"symbol\" in options:\n",
    "    whitelist = string.ascii_letters + string.digits + ' '\n",
    "    symbolRemoved=[]\n",
    "    for phrase in phrases:\n",
    "      phrase1=''.join(list(map(lambda cha: cha if cha  in whitelist else ' ',phrase)))\n",
    "      symbolRemoved.append(phrase1)\n",
    "    print(\"After symbol removal\")\n",
    "\n",
    "    phrases=symbolRemoved\n",
    "    print(phrases)\n",
    "\n",
    "\n",
    "  tokenized=[word_tokenize(phrase) for phrase in phrases] #split in individual words\n",
    "  # print(tokenized)\n",
    "  if \"stopword\" in options:\n",
    "    stop_words = set(stopwords.words('english')) #get set of stopwords\n",
    "    filtered=list(map(lambda phrase:[w for w in phrase if w not in stop_words],tokenized)) # remove words that appear in the stopwords set\n",
    "    print(\"After stopword removal\")\n",
    "\n",
    "    tokenized=filtered\n",
    "    #phrases=''.join(filtered)\n",
    "    phrases=[' '.join(x) for x in filtered]\n",
    "    print(phrases)\n",
    "  return phrases\n",
    "  print(\"Done\")\n",
    "\n",
    "# removeWords([\"This is-a really nice day to. ? swim in the ocean\",\"Hello  @my friends https://mdshasdasdhas carte\"],[\"stopword\",\"symbol\",\"link\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/home/maha/.local/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(1682, 2 - 1) = 1 components.\n  ChangedBehaviorWarning)\n/home/maha/.local/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n  warnings.warn(future_msg, FutureWarning)\n/home/maha/.local/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n  warnings.warn(\"Variables are collinear.\")\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.01601487 0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.03944497 0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.0306214  0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.02087293 0.02087293 0.02087293 0.02087293\n  0.02087293 0.02087293 0.02087293 0.02087293]\n [0.         0.         0.0208955  0.0208955  0.0208955  0.0208955\n  0.0208955  0.0208955  0.0208955  0.0208955 ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.02310644 0.         0.02084998 0.02084998 0.02084998 0.02084998\n  0.02084998 0.02084998 0.02084998 0.02084998]\n [0.         0.01564736 0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "def make_Dictionary(train_dir):\n",
    "    emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]\n",
    "    all_words = []\n",
    "    for mail in emails:\n",
    "        with open(mail) as m:\n",
    "            for i,line in enumerate(m):\n",
    "                if i == 2:  #Body of email is only 3rd line of text file\n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    " \n",
    "    dictionary1 = Counter(all_words)\n",
    "    # Paste code for non-word removal here(code snippet is given below)\n",
    "    list_to_remove = list(dictionary1)\n",
    "    \n",
    "    for item in list_to_remove:\n",
    "        if item.isalpha() == False:\n",
    "            del dictionary1[item]\n",
    "        elif len(item) == 1:\n",
    "            del dictionary1[item]\n",
    "    dictionary1 = dictionary1.most_common(3000)\n",
    "    return dictionary1\n",
    "def reduceFeatures(raw,nrFeats):\n",
    "    #min max strategy, cut from both ends until the required number of features\n",
    "    x=True\n",
    "    while(raw.shape[1]>nrFeats):\n",
    "        sums=[sum(x) for x in zip(*raw)]\n",
    "        index=-1\n",
    "        if x:\n",
    "            index=np.argmin(sums)\n",
    "        else:\n",
    "            index=np.argmax(sums)\n",
    "        x=not x\n",
    "        raw=np.delete(raw,index,1)\n",
    "    return raw\n",
    "        \n",
    "        \n",
    "def extract_my_features(mail_dir,filter):#idf features\n",
    "    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "    lines=[]   \n",
    "    for fil in files:\n",
    "        with open(fil) as fi:\n",
    "            for i,line in enumerate(fi):\n",
    "                if i == 2:\n",
    "                    lines.append(line)\n",
    "    # lines=[\"the house had a tiny little mouse\",\n",
    "    #   \"the cat saw the mouse\",\n",
    "    #   \"the mouse ran away from the house\",\n",
    "    #   \"the cat finally ate the mouse\",\n",
    "    #   \"the end of the mouse story\"]\n",
    "    lines=removeWords(lines,filter)\n",
    "    tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    "     \n",
    "    # just send in all your docs here\n",
    "    tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(lines)\n",
    "    first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n",
    " \n",
    "# place tf-idf values in a pandas data frame\n",
    "    df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "    df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "    \n",
    "    return tfidf_vectorizer_vectors\n",
    "def LDA_PCA(raw,option,nr_components,labels=None):\n",
    "    if option is \"pca\":\n",
    "        pca = PCA(n_components=nr_components)\n",
    "        x_pca = pca.fit_transform(raw)\n",
    "        return x_pca\n",
    "    if option is \"lda\":\n",
    "        lda = LDA(n_components=nr_components)\n",
    "        x_lda=lda.fit_transform(raw,labels)\n",
    "        return x_lda\n",
    "        \n",
    "def extract_features(mail_dir,dictio,filter):#based of number of occurences of words from dictionary\n",
    "    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "    features_matrix = np.zeros((len(files),3000))\n",
    "    docID = 0;\n",
    "   \n",
    "    for fil in files:\n",
    "      with open(fil) as fi:\n",
    "        for i,line in enumerate(fi):\n",
    "          if i == 2:\n",
    "            line=removeWords(line,filter)\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "             \n",
    "              for i,d in enumerate(dictio):\n",
    "                if d[0] == word:\n",
    "                  wordID = i\n",
    "                  features_matrix[docID,wordID] = words.count(word)\n",
    "        docID = docID + 1\n",
    "    return features_matrix\n",
    "# \n",
    "# train_dir = 'Datasets/ling-spam/train-mails'\n",
    "# features=extract_my_features(train_dir)\n",
    "# features_array=features.toarray()\n",
    "# pca_feat=LDA_PCA(features_array,\"pca\",2)\n",
    "# train_labels = np.ones(20)\n",
    "# train_labels[0:9] = 0\n",
    "# lda_feat=LDA_PCA(features_array,\"lda\",2,train_labels)\n",
    "# reduced_features=reduceFeatures(features_array,10)\n",
    "# print(reduced_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Initial\n['This is-a really nice day to. ? swim in the ocean', 'Hello  @my friends https://mdshasdasdhas carte']\nAfter link\n['This is-a really nice day to. ? swim in the ocean', 'Hello  @my friends   carte']\nAfter symbol removal\n['This is a really nice day to    swim in the ocean', 'Hello   my friends   carte']\nAfter stopword removal\n['This really nice day swim ocean', 'Hello friends carte']\nDone\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/maha/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /home/maha/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/home/maha/.local/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "[[ 4  5]\n [ 1 10]]\n[[2 7]\n [2 9]]\nThe final result for the classifiers Muntinomial|LinearSVC|Decision Tree=[[3 6]\n [3 8]]\nThe final result for the classifiers Muntinomial|Decision Tree|LinearSVC=[[3 6]\n [3 8]]\nThe final result for the classifiers LinearSVC|Muntinomial|Decision Tree=[[3 6]\n [3 8]]\nThe final result for the classifiers LinearSVC|Decision Tree|Muntinomial=[[3 6]\n [3 8]]\nThe final result for the classifiers Decision Tree|Muntinomial|LinearSVC=[[3 6]\n [3 8]]\nThe final result for the classifiers Decision Tree|LinearSVC|Muntinomial=[[3 6]\n [3 8]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from itertools import permutations \n",
    "from random import randint\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a dictionary of words with its frequency\n",
    "\n",
    "# Training SVM and Naive bayes classifier\n",
    "def train_classifiers(classifiers,train_matrix,train_labels):\n",
    "    for classifier in classifiers:\n",
    "        classifier.fit(train_matrix,train_labels)\n",
    "    return classifiers\n",
    "        \n",
    "def majority_voting(classifiers,classifier_labels,sample,true_labels):\n",
    "    results=[]\n",
    "    classifier_order=range(0,len(classifiers)-1);\n",
    "    k=3\n",
    "    combinations=permutations(classifier_order,k)\n",
    "    for index,i in enumerate(list(combinations)):\n",
    "        majority_vote=classifiers[i[0]].predict(sample)+classifiers[i[1]].predict(sample)+classifiers[i[2]].predict(sample);\n",
    "        final_result=np.round(majority_vote/k);\n",
    "        results.append(final_result)\n",
    "        print(\"The final result for the classifiers {}|{}|{}={}\".format(classifier_labels[i[0]],classifier_labels[i[1]],classifier_labels[i[2]],confusion_matrix(true_labels,final_result)))\n",
    "\n",
    " \n",
    "model1 = MultinomialNB()\n",
    "model2 = LinearSVC()\n",
    "model3 = tree.DecisionTreeClassifier()\n",
    "model4 = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "classifierArray=[model1,model2,model3,model4]\n",
    "classifierLabels=[\"Muntinomial\",\"LinearSVC\",\"Decision Tree\",\"Random Forest\"]\n",
    "\n",
    "\n",
    "train_dir = 'Datasets/ling-spam/train-mails'\n",
    "\n",
    "# Prepare feature vectors per training mail and its labels\n",
    " \n",
    "train_labels = np.ones(20)\n",
    "train_labels[0:9] = 0\n",
    "dictionary=make_Dictionary(train_dir) \n",
    "\n",
    "train_matrix = extract_features(train_dir,dictionary)\n",
    " \n",
    "classifiers=train_classifiers(classifierArray,train_matrix,train_labels)\n",
    "\n",
    "# Test the unseen mails for Spam\n",
    "test_dir = './Datasets/ling-spam/test-mails/'\n",
    "test_matrix = extract_features(test_dir,dictionary)\n",
    "test_labels = np.ones(20)\n",
    "test_labels[0:9] = 0\n",
    "result1 = model1.predict(test_matrix)\n",
    "result2 = model2.predict(test_matrix)\n",
    "print (confusion_matrix(test_labels,result1))\n",
    "print (confusion_matrix(test_labels,result2))\n",
    "majority_voting(classifierArray,classifierLabels,test_matrix,test_labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "The final result for the classifiers 1|2|3=0\nThe final result for the classifiers 1|3|2=1\nThe final result for the classifiers 2|1|3=1\nThe final result for the classifiers 2|3|1=0\nThe final result for the classifiers 3|1|2=0\nThe final result for the classifiers 3|2|1=0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "class mock_class:\n",
    "    def predict(self,s):\n",
    "        return randint(0,1)\n",
    "fake1=mock_class()\n",
    "fake2=mock_class()\n",
    "fake3=mock_class()\n",
    "fake4=mock_class()\n",
    "classSet=[fake1,fake2,fake3,fake4]\n",
    "classLabels=range(1,4)\n",
    "x=3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ]
}