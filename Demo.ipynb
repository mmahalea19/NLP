{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Demo.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "pycharm-ebd2ea66",
   "language": "python",
   "display_name": "PyCharm (NLP)"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mmahalea19/NLP/blob/master/Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LUXH7l86QVFR",
    "colab_type": "code",
    "outputId": "0e38b2b1-7e8b-4e46-bc6a-9f397587b81f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "NOTE!!! Keep the same folder structure W"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-1f500f859c75>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    NOTE!!! Keep the same folder structure W\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ],
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-1f500f859c75>, line 1)",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BpG_73RgQc9N",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "Task 1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(1682, 2 - 1) = 1 components.\n",
      "  if self.classes_.size == 2:  # treat binary case as a special case\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  def transform(self, X):\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-12da99ba537f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0mldm_feat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLDA_PCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"lda\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m \u001b[0mreduced_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduceFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-12da99ba537f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0mldm_feat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLDA_PCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"lda\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m \u001b[0mreduced_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduceFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/193.5662.61/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0;31m# if thread has a suspend flag, we suspend with a busy wait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydev_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSTATE_SUSPEND\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m                     \u001b[0;31m# No need to reset frame.f_trace to keep the same trace function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/193.5662.61/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001b[0m in \u001b[0;36mdo_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;31m# IFDEF CYTHON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/193.5662.61/plugins/python/helpers/pydev/pydevd.py\u001b[0m in \u001b[0;36mdo_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads_suspended_single_notification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify_thread_suspended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuspend_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_this_thread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuspend_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_this_thread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/193.5662.61/plugins/python/helpers/pydev/pydevd.py\u001b[0m in \u001b[0;36m_do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_internal_commands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel_async_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_current_thread_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "def make_Dictionary(train_dir):\n",
    "    emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]\n",
    "    all_words = []\n",
    "    for mail in emails:\n",
    "        with open(mail) as m:\n",
    "            for i,line in enumerate(m):\n",
    "                if i == 2:  #Body of email is only 3rd line of text file\n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    " \n",
    "    dictionary1 = Counter(all_words)\n",
    "    # Paste code for non-word removal here(code snippet is given below)\n",
    "    list_to_remove = list(dictionary1)\n",
    "    \n",
    "    for item in list_to_remove:\n",
    "        if item.isalpha() == False:\n",
    "            del dictionary1[item]\n",
    "        elif len(item) == 1:\n",
    "            del dictionary1[item]\n",
    "    dictionary1 = dictionary1.most_common(3000)\n",
    "    return dictionary1\n",
    "def reduceFeatures(raw,nrFeats):\n",
    "    #min max strategy\n",
    "    x=True\n",
    "    while(raw.shape[1]>nrFeats):\n",
    "        sums=[sum(x) for x in zip(*raw)]\n",
    "        index=-1\n",
    "        if x:\n",
    "            index=np.argmin(sums)\n",
    "        else:\n",
    "            index=np.argmax(sums)\n",
    "        x=not x\n",
    "        raw=np.delete(raw,index,1)\n",
    "    return raw\n",
    "        \n",
    "        \n",
    "def extract_my_features(mail_dir):\n",
    "    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "    features_matrix = np.zeros((len(files),3000))\n",
    "    docID = 0;\n",
    "    lines=[]\n",
    "    \n",
    "    \n",
    "        \n",
    "    # settings that you use for count vectorizer will go here\n",
    "   \n",
    "    for fil in files:\n",
    "        with open(fil) as fi:\n",
    "            for i,line in enumerate(fi):\n",
    "                if i == 2:\n",
    "                    lines.append(line)\n",
    "    # lines=[\"the house had a tiny little mouse\",\n",
    "    #   \"the cat saw the mouse\",\n",
    "    #   \"the mouse ran away from the house\",\n",
    "    #   \"the cat finally ate the mouse\",\n",
    "    #   \"the end of the mouse story\"]\n",
    "     \n",
    "    tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    "     \n",
    "    # just send in all your docs here\n",
    "    tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(lines)\n",
    "    first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n",
    " \n",
    "# place tf-idf values in a pandas data frame\n",
    "    df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "    df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "    \n",
    "    return tfidf_vectorizer_vectors\n",
    "def LDA_PCA(raw,option,nr_components,labels=None):\n",
    "    if option is \"pca\":\n",
    "        pca = PCA(n_components=nr_components)\n",
    "        x_pca = pca.fit_transform(raw)\n",
    "        return x_pca\n",
    "    if option is \"lda\":\n",
    "        lda = LDA(n_components=nr_components)\n",
    "        x_lda=lda.fit_transform(raw,labels)\n",
    "        return x_lda\n",
    "        \n",
    "def extract_features(mail_dir):\n",
    "    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "    features_matrix = np.zeros((len(files),3000))\n",
    "    docID = 0;\n",
    "    for fil in files:\n",
    "      with open(fil) as fi:\n",
    "        for i,line in enumerate(fi):\n",
    "          if i == 2:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "              wordID = 0\n",
    "              for i,d in enumerate(dictionary):\n",
    "                if d[0] == word:\n",
    "                  wordID = i\n",
    "                  features_matrix[docID,wordID] = words.count(word)\n",
    "        docID = docID + 1\n",
    "    return features_matrix\n",
    "\n",
    "train_dir = 'Datasets/ling-spam/train-mails'\n",
    "features=extract_my_features(train_dir)\n",
    "features_array=features.toarray()\n",
    "pca_feat=LDA_PCA(features_array,\"pca\",2)\n",
    "train_labels = np.ones(20)\n",
    "train_labels[0:9] = 0\n",
    "lda_feat=LDA_PCA(features_array,\"lda\",2,train_labels)\n",
    "reduced_features=reduceFeatures(features_array,10)\n",
    "print(reduced_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "[[2 7]\n",
      " [7 4]]\n",
      "[[ 3  6]\n",
      " [10  1]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a dictionary of words with its frequency\n",
    " \n",
    "train_dir = 'Datasets/ling-spam/train-mails'\n",
    "dictionary=make_Dictionary(train_dir) \n",
    "# Prepare feature vectors per training mail and its labels\n",
    " \n",
    "train_labels = np.ones(20)\n",
    "train_labels[0:9] = 0\n",
    "train_matrix = extract_features(train_dir)\n",
    " \n",
    "# Training SVM and Naive bayes classifier\n",
    "\n",
    " \n",
    "model1 = MultinomialNB()\n",
    "model2 = LinearSVC()\n",
    "model3 = tree.DecisionTreeClassifier()\n",
    "model4 = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "\n",
    "model1.fit(train_matrix,train_labels)\n",
    "model2.fit(train_matrix,train_labels)\n",
    "model3.fit(train_matrix, train_labels)\n",
    "model4.fit(train_matrix, train_labels)\n",
    "\n",
    " \n",
    "# Test the unseen mails for Spam\n",
    "test_dir = './Datasets/ling-spam/test-mails/'\n",
    "test_matrix = extract_features(test_dir)\n",
    "test_labels = np.ones(20)\n",
    "test_labels[0:9] = 0\n",
    "result1 = model1.predict(test_matrix)\n",
    "result2 = model2.predict(test_matrix)\n",
    "print (confusion_matrix(test_labels,result1))\n",
    "print (confusion_matrix(test_labels,result2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MfwkRWy-Yz1",
    "colab_type": "text"
   },
   "source": [
    "Task 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "The final result for the classifiers 1|2|3=0\n",
      "The final result for the classifiers 1|3|2=1\n",
      "The final result for the classifiers 2|1|3=0\n",
      "The final result for the classifiers 2|3|1=1\n",
      "The final result for the classifiers 3|1|2=1\n",
      "The final result for the classifiers 3|2|1=0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from itertools import permutations \n",
    "from random import randint\n",
    "class mock_class:\n",
    "    def predict(self,s):\n",
    "        return randint(0,1)\n",
    "        \n",
    "def majority_voting(classifiers,classifier_labels,sample):\n",
    "    results=[]\n",
    "    classifier_order=range(0,len(classifiers)-1);\n",
    "    k=3\n",
    "    combinations=permutations(classifier_order,k)\n",
    "    for index,i in enumerate(list(combinations)):\n",
    "        majority_vote=classifiers[i[0]].predict(sample)+classifiers[i[1]].predict(sample)+classifiers[i[2]].predict(sample);\n",
    "        final_result=round(majority_vote/k);\n",
    "        results.append(final_result)\n",
    "        print(\"The final result for the classifiers {}|{}|{}={}\".format(classifier_labels[i[0]],classifier_labels[i[1]],classifier_labels[i[2]],final_result))\n",
    "fake1=mock_class()\n",
    "fake2=mock_class()\n",
    "fake3=mock_class()\n",
    "fake4=mock_class()\n",
    "classSet=[fake1,fake2,fake3,fake4]\n",
    "classLabels=range(1,4)\n",
    "x=3\n",
    "majority_voting(classSet,classLabels,x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssF2WdoPWKA4",
    "colab_type": "text"
   },
   "source": [
    "NOTE!!! Keep the same folder structure as described below, when downloading the Enron dataset:\n",
    "\n",
    "Enron_PRE\n",
    "\n",
    "Enron_RAW"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Yzi8AZ_MWWKX",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FV8xEnwC4fA",
    "colab_type": "text"
   },
   "source": [
    "Task 4-remove stopwords and others\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zIcAZLmmC8c4",
    "colab_type": "code",
    "outputId": "7457a048-2513-4629-dbe0-3b5f42d6f97f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize ,RegexpTokenizer\n",
    "import string as string\n",
    "def removeWords(phrases,options):\n",
    "  print(\"Initial\")\n",
    "  print(phrases)\n",
    "  if(\"link\" in options):\n",
    "    phrases=[re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", phrase) for phrase in phrases]\n",
    "    print(\"After link\")\n",
    "    print(phrases)\n",
    "  if \"symbol\" in options:\n",
    "    whitelist = string.ascii_letters + string.digits + ' '\n",
    "    symbolRemoved=[]\n",
    "    for phrase in phrases:\n",
    "      phrase1=''.join(list(map(lambda cha: cha if cha  in whitelist else ' ',phrase)))\n",
    "      symbolRemoved.append(phrase1)\n",
    "    print(\"After symbol removal\")\n",
    "\n",
    "    phrases=symbolRemoved\n",
    "    print(phrases)\n",
    "\n",
    "\n",
    "  tokenized=[word_tokenize(phrase) for phrase in phrases] #split in individual words\n",
    "  print(tokenized)\n",
    "  if \"stopword\" in options:\n",
    "    stop_words = set(stopwords.words('english')) #get set of stopwords\n",
    "    filtered=list(map(lambda phrase:[w for w in phrase if w not in stop_words],tokenized)) # remove words that appear in the stopwords set\n",
    "    print(\"After stopword removal\")\n",
    "\n",
    "    tokenized=filtered\n",
    "    #phrases=''.join(filtered)\n",
    "    phrases=[' '.join(x) for x in filtered]\n",
    "    print(phrases)\n",
    "    print(\"Done\")\n",
    "\n",
    "removeWords([\"This is-a really nice day to. ? swim in the ocean\",\"Hello  @my friends https://mdshasdasdhas carte\"],[\"stopword\",\"symbol\",\"link\"])"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Initial\n",
      "['This is-a really nice day to. ? swim in the ocean', 'Hello  @my friends https://mdshasdasdhas carte']\n",
      "After link\n",
      "['This is-a really nice day to. ? swim in the ocean', 'Hello  @my friends   carte']\n",
      "After symbol removal\n",
      "['This is a really nice day to    swim in the ocean', 'Hello   my friends   carte']\n",
      "[['This', 'is', 'a', 'really', 'nice', 'day', 'to', 'swim', 'in', 'the', 'ocean'], ['Hello', 'my', 'friends', 'carte']]\n",
      "After stopword removal\n",
      "['This really nice day swim ocean', 'Hello friends carte']\n",
      "Done\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mihai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mihai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "re2S7W3CTGKv",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    ""
   ],
   "execution_count": 13,
   "outputs": []
  }
 ]
}